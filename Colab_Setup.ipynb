{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c5bdb6",
   "metadata": {},
   "source": [
    "# üöÄ Hybrid Mamba-xLSTM: Google Colab Setup\n",
    "\n",
    "Complete setup and training guide for Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d24310",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b8366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU available. Please enable GPU in Runtime settings!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and install the project and test it out \n",
    "!git clone https://github.com/krishankb-de/hybrid_model_mamba_xlstm.git\n",
    "%cd hybrid_model_mamba_xlstm\n",
    "!pip install -e . -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015b9a35",
   "metadata": {},
   "source": [
    "## Step 2: Mount Google Drive (Optional, for saving checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1351b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úì Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ee01d6",
   "metadata": {},
   "source": [
    "## Step 3: Quick Test (2 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d04179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '/content/hybrid_model_mamba_xlstm')\n",
    "from hybrid_xmamba.models.configuration_hybrid import HybridConfig\n",
    "from hybrid_xmamba.models.hybrid_lm import HybridLanguageModel\n",
    "\n",
    "print('Loading model...')\n",
    "config = HybridConfig(\n",
    "    dim=1024,\n",
    "    num_layers=24,\n",
    "    vocab_size=50257,\n",
    "    state_size=16,\n",
    "    conv_size=4,\n",
    "    expand_factor=2,\n",
    "    use_fast_path=True,\n",
    "    head_dim=64,\n",
    "    num_heads=16,\n",
    "    use_tfla=True,\n",
    "    proj_factor=2,\n",
    "    slstm_hidden_dim=1024,\n",
    "    slstm_num_heads=4,\n",
    "    use_exponential_gate=True,\n",
    ")\n",
    "\n",
    "model = HybridLanguageModel(config).eval().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "print('Testing inference...')\n",
    "text = 'The quick brown fox'\n",
    "inputs = tokenizer(text, return_tensors='pt').to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "print(f'‚úì Model loaded and inference works!')\n",
    "print(f'Output shape: {outputs.logits.shape}')\n",
    "print(f'Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M')\n",
    "print('‚úÖ Quick test completed successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d63a7",
   "metadata": {},
   "source": [
    "## Step 4: Full Training (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31397561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training (for local machine with GPU, not recommended for Colab free tier)\n",
    "# Uncomment and run this on a machine with >= 24GB GPU VRAM\n",
    "# Note: The 350M model is too large for Colab T4 (15GB). \n",
    "# For Colab training, consider using a smaller model or local machine.\n",
    "\n",
    "# !python scripts/train.py \\\n",
    "#     model=hybrid_350m \\\n",
    "#     dataset=wikitext \\\n",
    "#     trainer=colab_single_gpu \\\n",
    "#     trainer.max_epochs=3 \\\n",
    "#     trainer.default_root_dir=/content/drive/MyDrive/hybrid_mamba_checkpoints \\\n",
    "#     dataset.batch_size=4 \\\n",
    "#     dataset.eval_batch_size=4 \\\n",
    "#     dataset.num_workers=0 \\\n",
    "#     +dataset.max_seq_length=128 \\\n",
    "#     wandb.enabled=false\n",
    "\n",
    "print(\"üìù For full training on Colab, use a smaller model or local GPU\")\n",
    "print(\"‚úÖ Step 3 (inference test) completed successfully!\")\n",
    "print(\"üöÄ To train locally: python scripts/train.py model=hybrid_350m dataset=wikitext trainer=single_gpu\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
