{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c5bdb6",
   "metadata": {},
   "source": [
    "# üöÄ Hybrid Mamba-xLSTM: Google Colab Setup\n",
    "\n",
    "Complete setup and training guide for Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d24310",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b8366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU available. Please enable GPU in Runtime settings!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and install the project and test it out \n",
    "!git clone https://github.com/krishankb-de/hybrid_model_mamba_xlstm.git\n",
    "%cd hybrid_model_mamba_xlstm\n",
    "!pip install -e . -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015b9a35",
   "metadata": {},
   "source": [
    "## Step 2: Mount Google Drive (Optional, for saving checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1351b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úì Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ee01d6",
   "metadata": {},
   "source": [
    "## Step 3: Quick Test (2 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d04179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inference test with 70M model (completes in < 2 minutes)\n",
    "python_script = \"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import sys\n",
    "sys.path.insert(0, '/content/hybrid_model_mamba_xlstm')\n",
    "\n",
    "from hybrid_xmamba.models.configuration_hybrid import HybridConfig\n",
    "from hybrid_xmamba.models.hybrid_lm import HybridLanguageModel\n",
    "\n",
    "print('Loading 70M model...')\n",
    "config = HybridConfig(\n",
    "    dim=512,\n",
    "    num_layers=8,\n",
    "    vocab_size=50257,\n",
    "    state_size=16,\n",
    "    conv_size=4,\n",
    "    expand_factor=2,\n",
    "    use_fast_path=True,\n",
    "    head_dim=64,\n",
    "    num_heads=8,\n",
    "    use_tfla=True,\n",
    "    proj_factor=2,\n",
    "    slstm_hidden_dim=512,\n",
    "    slstm_num_heads=4,\n",
    "    use_exponential_gate=True,\n",
    ")\n",
    "\n",
    "model = HybridLanguageModel(config).eval().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "print('Testing inference...')\n",
    "text = 'The quick brown fox jumps over the lazy dog'\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Pass only input_ids to the model\n",
    "    outputs = model(input_ids=inputs['input_ids'].cuda())\n",
    "\n",
    "print(f'‚úì Model loaded and inference works!')\n",
    "print(f'Output shape: {outputs.logits.shape}')\n",
    "print(f'Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M')\n",
    "print('‚úÖ Quick test completed successfully!')\n",
    "\"\"\"\n",
    "\n",
    "!python -c \"$python_script\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f520a95",
   "metadata": {},
   "source": [
    "### For 350M complete model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19fc32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training (for local machine with GPU, not recommended for Colab free tier)\n",
    "# Uncomment and run this on a machine with >= 24GB GPU VRAM\n",
    "# Note: The 350M model is too large for Colab T4 (15GB). \n",
    "# For Colab training, consider using a smaller model or local machine.\n",
    "\n",
    "# !python scripts/train.py \\\n",
    "#     model=hybrid_350m \\\n",
    "#     dataset=wikitext \\\n",
    "#     trainer=colab_single_gpu \\\n",
    "#     trainer.max_epochs=3 \\\n",
    "#     trainer.default_root_dir=/content/drive/MyDrive/hybrid_mamba_checkpoints \\\n",
    "#     dataset.batch_size=4 \\\n",
    "#     dataset.eval_batch_size=4 \\\n",
    "#     dataset.num_workers=0 \\\n",
    "#     +dataset.max_seq_length=128 \\\n",
    "#     wandb.enabled=false\n",
    "\n",
    "# print(\"üìù For full training on Colab, use a smaller model or local GPU\")\n",
    "# print(\"‚úÖ Step 3 (inference test) completed successfully!\")\n",
    "# print(\"üöÄ To train locally: python scripts/train.py model=hybrid_350m dataset=wikitext trainer=single_gpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d63a7",
   "metadata": {},
   "source": [
    "## Step 4: Full Training (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31397561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Train 70M Hybrid Model (Recommended for Colab T4)\n",
    "# The 70M model fits in Colab's 15GB GPU memory\n",
    "# Training will take ~2-3 hours per epoch on T4\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ TRAINING HYBRID 70M MODEL ON COLAB\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nModel: hybrid_70m (70M parameters)\")\n",
    "print(\"GPU: Colab T4 (15GB VRAM)\")\n",
    "print(\"Estimated time per epoch: 2-3 hours\")\n",
    "print(\"Dataset: WikiText-103 (1.8M training examples)\")\n",
    "print(\"\\nRunning training... This may take a while.\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Train the 70M hybrid model with optimized Colab settings\n",
    "!python scripts/train.py \\\n",
    "    model=hybrid_70m \\\n",
    "    dataset=wikitext \\\n",
    "    trainer=colab_single_gpu \\\n",
    "    trainer.max_epochs=1 \\\n",
    "    trainer.num_sanity_val_steps=0 \\\n",
    "    dataset.batch_size=4 \\\n",
    "    dataset.eval_batch_size=4 \\\n",
    "    dataset.num_workers=2 \\\n",
    "    dataset.preprocessing_num_workers=2 \\\n",
    "    +dataset.max_seq_length=256 \\\n",
    "    trainer.accumulate_grad_batches=2 \\\n",
    "    trainer.val_check_interval=0.5 \\\n",
    "    trainer.log_every_n_steps=20 \\\n",
    "    wandb.enabled=false \\\n",
    "    trainer.enable_checkpointing=true \\\n",
    "    trainer.default_root_dir=/content/outputs\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Training completed!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Optional: Copy results to Google Drive for persistence\n",
    "!mkdir -p /content/drive/MyDrive/hybrid_mamba_results 2>/dev/null\n",
    "!cp -r /content/outputs /content/drive/MyDrive/hybrid_mamba_results/ 2>/dev/null && echo \"Results saved to Google Drive\" || echo \"Could not save to Drive (not mounted)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Train 150M Hybrid Model (More compute-intensive, may need gradient checkpointing)\n",
    "# Only use this if you want to train a larger model\n",
    "# May result in OOM on T4 - use gradient checkpointing or reduce batch size if needed\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ TRAINING HYBRID 150M MODEL ON COLAB\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nModel: hybrid_150m (150M parameters)\")\n",
    "print(\"GPU: Colab T4 (15GB VRAM)\")\n",
    "print(\"Estimated time per epoch: 3-4 hours (slower due to size)\")\n",
    "print(\"Dataset: WikiText-103\")\n",
    "print(\"\\nNote: This is more resource-intensive. If you get OOM errors:\")\n",
    "print(\"  - Reduce batch_size to 2\")\n",
    "print(\"  - Reduce +dataset.max_seq_length to 128\")\n",
    "print(\"  - Increase trainer.accumulate_grad_batches to 4\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Train the 150M hybrid model\n",
    "!python scripts/train.py \\\n",
    "    model=hybrid_150m \\\n",
    "    dataset=wikitext \\\n",
    "    trainer=colab_single_gpu \\\n",
    "    trainer.max_epochs=1 \\\n",
    "    trainer.num_sanity_val_steps=0 \\\n",
    "    dataset.batch_size=2 \\\n",
    "    dataset.eval_batch_size=2 \\\n",
    "    dataset.num_workers=2 \\\n",
    "    dataset.preprocessing_num_workers=2 \\\n",
    "    +dataset.max_seq_length=256 \\\n",
    "    trainer.accumulate_grad_batches=2 \\\n",
    "    trainer.val_check_interval=0.5 \\\n",
    "    trainer.log_every_n_steps=20 \\\n",
    "    wandb.enabled=false \\\n",
    "    trainer.enable_checkpointing=true \\\n",
    "    trainer.default_root_dir=/content/outputs\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Training completed!\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
