{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c5bdb6",
   "metadata": {},
   "source": [
    "# üöÄ Hybrid Mamba-xLSTM: Google Colab Setup\n",
    "\n",
    "Complete setup and training guide for Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d24310",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b8366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU available. Please enable GPU in Runtime settings!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and install the project and test it out \n",
    "!git clone https://github.com/krishankb-de/hybrid_model_mamba_xlstm.git\n",
    "%cd hybrid_model_mamba_xlstm\n",
    "!pip install -e . -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015b9a35",
   "metadata": {},
   "source": [
    "## Step 2: Mount Google Drive (Optional, for saving checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1351b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úì Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ee01d6",
   "metadata": {},
   "source": [
    "## Step 3: Quick Test (2 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d04179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inference test with 70M model (completes in < 2 minutes)\n",
    "python_script = \"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import sys\n",
    "sys.path.insert(0, '/content/hybrid_model_mamba_xlstm')\n",
    "\n",
    "from hybrid_xmamba.models.configuration_hybrid import HybridConfig\n",
    "from hybrid_xmamba.models.hybrid_lm import HybridLanguageModel\n",
    "\n",
    "print('Loading 70M model...')\n",
    "config = HybridConfig(\n",
    "    dim=512,\n",
    "    num_layers=8,\n",
    "    vocab_size=50257,\n",
    "    state_size=16,\n",
    "    conv_size=4,\n",
    "    expand_factor=2,\n",
    "    use_fast_path=True,\n",
    "    head_dim=64,\n",
    "    num_heads=8,\n",
    "    use_tfla=True,\n",
    "    proj_factor=2,\n",
    "    slstm_hidden_dim=512,\n",
    "    slstm_num_heads=4,\n",
    "    use_exponential_gate=True,\n",
    ")\n",
    "\n",
    "model = HybridLanguageModel(config).eval().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "print('Testing inference...')\n",
    "text = 'The quick brown fox jumps over the lazy dog'\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Pass only input_ids to the model\n",
    "    outputs = model(input_ids=inputs['input_ids'].cuda())\n",
    "\n",
    "print(f'‚úì Model loaded and inference works!')\n",
    "print(f'Output shape: {outputs.logits.shape}')\n",
    "print(f'Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M')\n",
    "print('‚úÖ Quick test completed successfully!')\n",
    "\"\"\"\n",
    "\n",
    "!python -c \"$python_script\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f520a95",
   "metadata": {},
   "source": [
    "### For 350M complete model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19fc32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training (for local machine with GPU, not recommended for Colab free tier)\n",
    "# Uncomment and run this on a machine with >= 24GB GPU VRAM\n",
    "# Note: The 350M model is too large for Colab T4 (15GB). \n",
    "# For Colab training, consider using a smaller model or local machine.\n",
    "\n",
    "# !python scripts/train.py \\\n",
    "#     model=hybrid_350m \\\n",
    "#     dataset=wikitext \\\n",
    "#     trainer=colab_single_gpu \\\n",
    "#     trainer.max_epochs=3 \\\n",
    "#     trainer.default_root_dir=/content/drive/MyDrive/hybrid_mamba_checkpoints \\\n",
    "#     dataset.batch_size=4 \\\n",
    "#     dataset.eval_batch_size=4 \\\n",
    "#     dataset.num_workers=0 \\\n",
    "#     +dataset.max_seq_length=128 \\\n",
    "#     wandb.enabled=false\n",
    "\n",
    "# print(\"üìù For full training on Colab, use a smaller model or local GPU\")\n",
    "# print(\"‚úÖ Step 3 (inference test) completed successfully!\")\n",
    "# print(\"üöÄ To train locally: python scripts/train.py model=hybrid_350m dataset=wikitext trainer=single_gpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d63a7",
   "metadata": {},
   "source": [
    "## Step 4: Full Training (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31397561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è IMPORTANT: Colab 12-hour Timeout Limitation\n",
    "# Training the full epoch takes 130+ days on Colab (450k batches √ó 25 sec/batch)\n",
    "# This cell provides THREE options:\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚ö†Ô∏è  COLAB TRAINING OPTIONS - Choose One\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nColab Free Tier: 12-hour maximum runtime\")\n",
    "print(\"Model speed: ~25 seconds per batch\")\n",
    "print(\"Full training needed: 450,338 batches = 130+ days ‚ùå\")\n",
    "print(\"\\nChoose an option below:\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: QUICK TRAINING TEST (10-15 minutes) ‚≠ê RECOMMENDED FOR COLAB\n",
    "# This runs only 100 training steps to verify everything works\n",
    "# Perfect for testing code, data loading, and model without timeout risk\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ OPTION 1: QUICK TRAINING TEST (100 steps = 10-15 minutes)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nPurpose: Verify training works end-to-end\")\n",
    "print(\"Runtime: ~10-15 minutes (safe within 12-hour limit)\")\n",
    "print(\"Expected loss reduction: Should see training progress\\n\")\n",
    "\n",
    "!python scripts/train.py \\\n",
    "    model=hybrid_70m \\\n",
    "    dataset=wikitext \\\n",
    "    trainer=colab_single_gpu \\\n",
    "    trainer.max_epochs=1 \\\n",
    "    trainer.num_sanity_val_steps=0 \\\n",
    "    dataset.batch_size=4 \\\n",
    "    dataset.eval_batch_size=4 \\\n",
    "    dataset.num_workers=2 \\\n",
    "    dataset.preprocessing_num_workers=2 \\\n",
    "    +dataset.max_seq_length=256 \\\n",
    "    trainer.accumulate_grad_batches=2 \\\n",
    "    trainer.val_check_interval=0.5 \\\n",
    "    trainer.log_every_n_steps=10 \\\n",
    "    trainer.limit_train_batches=100 \\\n",
    "    trainer.limit_val_batches=0 \\\n",
    "    wandb.enabled=false \\\n",
    "    trainer.enable_checkpointing=false \\\n",
    "    trainer.default_root_dir=/content/outputs\n",
    "\n",
    "print(\"\\n‚úÖ Quick test completed! Model trains successfully.\")\n",
    "print(\"üìä Check output folder at /content/outputs for results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd34c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: EXTENDED TRAINING (1000 steps = ~6-7 hours)\n",
    "# Trains for longer but still completes within Colab's 12-hour limit\n",
    "# Good for getting meaningful model improvements\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ OPTION 2: EXTENDED TRAINING (1000 steps = 6-7 hours)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nPurpose: Train for meaningful results\")\n",
    "print(\"Runtime: ~6-7 hours (safe with headroom)\")\n",
    "print(\"Coverage: ~0.2% of full epoch\")\n",
    "print(\"Recommendation: Use this if you have time and want real training\\n\")\n",
    "\n",
    "!python scripts/train.py \\\n",
    "    model=hybrid_70m \\\n",
    "    dataset=wikitext \\\n",
    "    trainer=colab_single_gpu \\\n",
    "    trainer.max_epochs=1 \\\n",
    "    trainer.num_sanity_val_steps=0 \\\n",
    "    dataset.batch_size=4 \\\n",
    "    dataset.eval_batch_size=4 \\\n",
    "    dataset.num_workers=2 \\\n",
    "    dataset.preprocessing_num_workers=2 \\\n",
    "    +dataset.max_seq_length=256 \\\n",
    "    trainer.accumulate_grad_batches=2 \\\n",
    "    trainer.val_check_interval=0.5 \\\n",
    "    trainer.log_every_n_steps=50 \\\n",
    "    trainer.limit_train_batches=1000 \\\n",
    "    trainer.limit_val_batches=0 \\\n",
    "    wandb.enabled=false \\\n",
    "    trainer.enable_checkpointing=true \\\n",
    "    trainer.default_root_dir=/content/outputs\n",
    "\n",
    "print(\"\\n‚úÖ Extended training completed!\")\n",
    "print(\"üíæ Checkpoint saved to /content/outputs\")\n",
    "print(\"üìä Training artifacts ready for use\")\n",
    "\n",
    "# Save to Google Drive\n",
    "!mkdir -p /content/drive/MyDrive/hybrid_mamba_results 2>/dev/null\n",
    "!cp -r /content/outputs /content/drive/MyDrive/hybrid_mamba_results/ 2>/dev/null && \\\n",
    "    echo \"‚úì Results saved to Google Drive\" || echo \"‚ö†Ô∏è Could not save to Drive (not mounted)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd57aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3: FULL TRAINING ON LOCAL MACHINE (Recommended for real training)\n",
    "# For complete training, use your local GPU with 24GB+ VRAM\n",
    "# This is the only practical way to train the full model\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíª OPTION 3: FULL TRAINING ON LOCAL MACHINE (RECOMMENDED)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "Colab is NOT suitable for full model training due to:\n",
    "  ‚úó 12-hour runtime limit\n",
    "  ‚úó 450k+ batches needed for full epoch\n",
    "  ‚úó At 25s/batch = 130+ days total time\n",
    "\n",
    "SOLUTION: Train locally with a better GPU\n",
    "\n",
    "Requirements:\n",
    "  - GPU: 24GB+ VRAM (RTX 3090, RTX 4090, A100, etc.)\n",
    "  - Storage: 50GB free disk space\n",
    "  - Time: ~3-4 hours per epoch on RTX 4090\n",
    "\n",
    "Installation (run on your local machine):\n",
    "  1. Clone the repository\n",
    "  2. pip install -e .\n",
    "  3. Run the training command below\n",
    "\n",
    "TRAINING COMMANDS FOR LOCAL MACHINE:\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Single GPU Training (24GB VRAM - RTX 3090/4090):\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "python scripts/train.py \\\\\n",
    "    model=hybrid_70m \\\\\n",
    "    dataset=wikitext \\\\\n",
    "    trainer=single_gpu \\\\\n",
    "    trainer.max_epochs=10 \\\\\n",
    "    dataset.batch_size=8 \\\\\n",
    "    +dataset.max_seq_length=256 \\\\\n",
    "    trainer.accumulate_grad_batches=1 \\\\\n",
    "    wandb.enabled=false\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Multi-GPU Training (Distributed - faster):\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "python scripts/train.py \\\\\n",
    "    model=hybrid_70m \\\\\n",
    "    dataset=wikitext \\\\\n",
    "    trainer=gpu_ddp \\\\\n",
    "    trainer.max_epochs=10 \\\\\n",
    "    dataset.batch_size=8 \\\\\n",
    "    +dataset.max_seq_length=256 \\\\\n",
    "    wandb.enabled=false\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"For 150M or 350M models (requires more VRAM):\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "python scripts/train.py \\\\\n",
    "    model=hybrid_150m \\\\\n",
    "    dataset=wikitext \\\\\n",
    "    trainer=single_gpu \\\\\n",
    "    trainer.max_epochs=5 \\\\\n",
    "    dataset.batch_size=4 \\\\\n",
    "    trainer.accumulate_grad_batches=2\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ This is the recommended approach for actual training\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
