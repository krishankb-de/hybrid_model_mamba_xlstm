# 7B parameter hybrid model configuration
# Suitable for large-scale training on multiple GPUs

model_type: "hybrid_lm"

# Architecture
vocab_size: 50257
dim: 4096
num_layers: 32
layer_pattern: ["mamba", "mamba", "mlstm"]  # Repeat pattern

# Mamba parameters
state_size: 16
conv_size: 4
expand_factor: 2
dt_rank: null  # Auto-computed
use_fast_path: true

# mLSTM parameters
head_dim: 128
num_heads: 32  # 4096 / 128 = 32
use_tfla: true
proj_factor: 2

# sLSTM parameters
slstm_hidden_dim: 4096
slstm_num_heads: 4
use_exponential_gate: true

# Shared parameters
norm_type: "rms"
use_mlp: true
mlp_ratio: 4.0

# Training parameters
max_position_embeddings: 4096
dropout: 0.1
initializer_range: 0.02

# Generation parameters
use_cache: true
tie_word_embeddings: false

# Optimization
learning_rate: 3.0e-4
weight_decay: 0.1
warmup_steps: 2000
max_steps: 100000
gradient_clip_val: 1.0
