# 70M parameter hybrid model configuration
# Suitable for Google Colab and resource-constrained environments
# Maintains the same Mamba-xLSTM hybrid architecture as larger models

model_type: "hybrid_lm"

# Architecture
vocab_size: 50257
dim: 512          # Reduced from 1024 (scales parameters quadratically)
num_layers: 8     # Reduced from 24 (maintains layer pattern)
layer_pattern: ["mamba", "mamba", "mlstm"]

# Mamba parameters
state_size: 16    # Keep same for quality
conv_size: 4      # Keep same
expand_factor: 2  # Keep same
dt_rank: null     # Auto: 512 / 16 = 32
use_fast_path: true

# mLSTM parameters
head_dim: 64      # Keep same for attention head quality
num_heads: 8      # Reduced from 16 (512 / 64 = 8)
use_tfla: true    # Keep efficient attention
proj_factor: 2    # Keep same

# sLSTM parameters
slstm_hidden_dim: 512  # Reduced from 1024
slstm_num_heads: 4     # Keep same
use_exponential_gate: true

# Shared parameters
norm_type: "rms"
use_mlp: true
mlp_ratio: 4.0

# Training parameters
max_position_embeddings: 2048
dropout: 0.1
initializer_range: 0.02

# Generation parameters
use_cache: true
tie_word_embeddings: false

# Optimization
learning_rate: 6.0e-4
weight_decay: 0.1
warmup_steps: 1000
max_steps: 50000
gradient_clip_val: 1.0
