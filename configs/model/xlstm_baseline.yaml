# Pure xLSTM (mLSTM) baseline configuration
# For comparison against hybrid architecture

model_type: "hybrid_lm"

# Architecture
vocab_size: 50257
dim: 2048
num_layers: 48
layer_pattern: ["mlstm"]  # Pure mLSTM, no Mamba layers

# Mamba parameters (not used but required by config)
state_size: 16
conv_size: 4
expand_factor: 2
dt_rank: null
use_fast_path: true

# mLSTM parameters
head_dim: 64
num_heads: 32  # 2048 / 64 = 32
use_tfla: true
proj_factor: 2

# sLSTM parameters (not used)
slstm_hidden_dim: 2048
slstm_num_heads: 4
use_exponential_gate: true

# Shared parameters
norm_type: "rms"
use_mlp: true
mlp_ratio: 4.0

# Training parameters
max_position_embeddings: 4096
dropout: 0.1
initializer_range: 0.02

# Generation parameters
use_cache: true
tie_word_embeddings: false

# Optimization
learning_rate: 3.0e-4
weight_decay: 0.1
warmup_steps: 2000
max_steps: 100000
gradient_clip_val: 1.0
