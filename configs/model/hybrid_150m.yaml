# 150M parameter hybrid model configuration
# Suitable for Google Colab Pro and medium-range GPUs
# Maintains the same Mamba-xLSTM hybrid architecture as larger models

model_type: "hybrid_lm"

# Architecture
vocab_size: 50257
dim: 768          # Between 512 and 1024
num_layers: 12    # Half of 24
layer_pattern: ["mamba", "mamba", "mlstm"]

# Mamba parameters
state_size: 16    # Keep same for quality
conv_size: 4      # Keep same
expand_factor: 2  # Keep same
dt_rank: null     # Auto: 768 / 16 = 48
use_fast_path: true

# mLSTM parameters
head_dim: 64      # Keep same for attention head quality
num_heads: 12     # 768 / 64 = 12
use_tfla: true    # Keep efficient attention
proj_factor: 2    # Keep same

# sLSTM parameters
slstm_hidden_dim: 768   # Scaled from 1024
slstm_num_heads: 4      # Keep same
use_exponential_gate: true

# Shared parameters
norm_type: "rms"
use_mlp: true
mlp_ratio: 4.0

# Training parameters
max_position_embeddings: 2048
dropout: 0.1
initializer_range: 0.02

# Generation parameters
use_cache: true
tie_word_embeddings: false

# Optimization
learning_rate: 6.0e-4
weight_decay: 0.1
warmup_steps: 1000
max_steps: 50000
gradient_clip_val: 1.0
